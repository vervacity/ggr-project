#!/usr/bin/env python

"""Runs GGR analysis
"""

import os
import sys
import logging
import subprocess
import argparse
import glob
import pkg_resources
import json


def parse_args():
    """Set up arguments
    """
    parser = argparse.ArgumentParser(
        description='Run GGR analysis')
    parser.add_argument(
        "-o", "--out_dir", dest="out_dir", type=str, default=os.getcwd(),
        help = "Output directory (default: current dir)")
    parser.add_argument(
        "--cluster", type=str, default="kundaje",
        help = "Cluster where analysis is running (sets up files correctly)")
    parser.add_argument(
        "--threads", type=int, default=12,
        help="number of threads for parallel processes")
    
    args = parser.parse_args()
    
    return args


def setup_folders(args):
    """Just make all the folders up front 
    so that don't have to worry about it
    """
    for folder_key in args.folders.keys():
        full_folder_path = "{0}/{1}".format(
            args.out_dir, args.folders[folder_key])
        args.folders[folder_key] = full_folder_path
        os.system("mkdir -p {0}".format(
            full_folder_path))
        
    return None


def track_runs(args):
    """track command and github commit
    """
    logging_file = "{}/ggr-project.log".format(args.out_dir)
    
    # track github commit
    git_repo_path = os.path.dirname(os.path.realpath(__file__))
    os.system('echo "commit:" > {0}'.format(logging_file))
    os.system('git --git-dir={0}/.git rev-parse HEAD >> {1}'.format(
        git_repo_path.split("/bin")[0], logging_file))
    os.system('echo "" >> {0}'.format(logging_file))
    
    # write out the command
    with open(logging_file, 'a') as f:
        f.write(' '.join(sys.argv)+'\n\n')
        
    return logging_file


def main():
    """Run all workflows across GGR datasets
    """
    # parse args
    args = parse_args()
    os.system("mkdir -p {}".format(args.out_dir))

    # set up inputs
    args.inputs = {}
    json_files = glob.glob(
        pkg_resources.resource_filename('ggr', 'data/*.json'))
    for json_file in json_files:
        key_name = os.path.basename(json_file).split('.json')[0]
        with open(json_file, 'r') as fp:
            args.inputs[key_name] = json.load(fp)

    # load the dataset summary file
    args.dataset_summary = pkg_resources.resource_filename(
        "ggr", "data/ggr_datasets.txt")
            
    # make output dict to track files
    args.outputs = {}
    
    # set up logging
    logging_file = track_runs(args)
    reload(logging) # reload because Anaconda co-opts logging (i think)
    logging.basicConfig(filename=logging_file, level=logging.DEBUG)
    logging.getLogger().addHandler(logging.StreamHandler())

    # set up top level: annotations, data, results
    toplevel_dirnames = ["annotations", "data", "results"]
    for dirname in toplevel_dirnames:
        args.outputs[dirname] = {
            "dir": "{}/{}".format(args.out_dir, dirname)}

    # and also high level keep track of directories with clusterings
    # to put into deep learning model
    args.outputs["results"]["label_dirs"] = []
    
    # run annotations workflow
    from ggr.workflows.annotations import runall
    args = runall(args, "hg19")

    # run atac workflow
    from ggr.workflows.atac import runall
    args = runall(args, "ggr.atac")
    
    # run histone workflow
    from ggr.workflows.histones import runall
    args = runall(args, "ggr.histone")

    # run rna workflow
    from ggr.workflows.rna import runall
    args = runall(args, "ggr.rna")

    # run epigenome workflow
    from ggr.workflows.epigenome import runall
    args = runall(args, "ggr.epigenome")

    # run deep learning workflow
    from ggr.workflows.learning import runall
    args = runall(args, "ggr")
    
    # save outputs dict to json
    with open("outputs.json", "w") as fp:
        json.dump(args.outputs, fp, sort_keys=True, indent=4)

    return None


if __name__ == '__main__':
        main()
